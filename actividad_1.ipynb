{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b07c13a5c4541140dcfeb22fadf752bd",
     "grade": false,
     "grade_id": "cell-570cf80ae1b2c48e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Actividad 1: Pipeline de procesamiento de datos con HDFS y Spark\n",
    "\n",
    "**Actividad del Proyecto (actividad grupal)**\n",
    "\n",
    "Esta actividad está asociada al Proyecto transversal del título y para su desarrollo, tendrás que utilizar obligatoriamente el siguiente recurso:\n",
    "* Dataset flights del Catálogo de Datos del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuerda borrar siempre las líneas que dicen `raise NotImplementedError`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a238d899b5a6e8c414330ade880233f",
     "grade": false,
     "grade_id": "cell-f4c598b6fd61ee12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Lee con detenimiento cada ejercicio. Las variables utilizadas para almacenar las soluciones, al igual que las nuevas columnas creadas, deben llamarse **exactamente** como indica el ejercicio, o de lo contrario los tests fallarán y el ejercicio no puntuará. Debe reemplazarse el valor `None` al que están inicializadas por el código necesario para resolver el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db4faded3e277c6e6253d9a9ba99f2d9",
     "grade": false,
     "grade_id": "cell-42368b0202b6ce77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Leemos el fichero flights.csv que hemos subido a Google Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "219cb0eba4e188af1ef349ee35d7d334",
     "grade": false,
     "grade_id": "cell-3202a483f423590a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Aunque para las capturas de pantalla se pide subir el fichero a HDFS, el resto de la actividad puede hacerse leyendo el mismo fichero que hemos subido al bucket de Google Cloud Storage.\n",
    "\n",
    "Indicamos que contiene encabezados (nombres de columnas) y que intente inferir el esquema, aunque después comprobaremos si lo\n",
    "ha inferido correctamente o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  done\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849827 sha256=460a07e2f426bd8ab91f8e1ff34f7be94c71b9bc1694a77391b4bbf34d58c871\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/13/92/64/da92a3521323cc629fdf25dd56eb26938e08014c1b57ad3759\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/16 03:15:25 WARN Utils: Your hostname, codespaces-73ce61 resolves to a loopback address: 127.0.0.1; using 10.0.4.141 instead (on interface eth0)\n",
      "25/01/16 03:15:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/16 03:15:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 32974)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flights Data Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50a7e93a1c2653ab52ccfbaf70c0edbb",
     "grade": false,
     "grade_id": "lectura-fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "ruta_gcs = \"datos/flights.csv\" # Reemplaza esto por la ruta correcta del fichero flights.csv en tu bucket de Google Cloud Storage\n",
    "flightsDF = None\n",
    "\n",
    "# Descomentar estas líneas\n",
    "flightsDF = spark.read\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .csv(ruta_gcs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos el esquema para comprobar qué tipo de dato ha inferido en cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el número de filas que tiene el DataFrame para hacernos una idea de su tamaño:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162049"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos 162049 filas. Si imprimimos por pantalla las 5 primeras filas, veremos qué tipos parecen tener y en qué columnas no coincide el tipo que podríamos esperar con el tipo que ha inferido Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|      -23|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|       43|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como\n",
    "*no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna\n",
    "el tipo de dato string (cadena de caracteres). Concretamente, las siguientes columnas deberían ser de tipo entero pero Spark\n",
    "las muestra como string:\n",
    "<ul>\n",
    " <li>dep_time: string (nullable = true)\n",
    " <li>dep_delay: string (nullable = true)\n",
    " <li>arr_time: string (nullable = true)\n",
    " <li>arr_delay: string (nullable = true)\n",
    " <li>air_time: string (nullable = true)\n",
    " <li>hour: string (nullable = true)\n",
    " <li>minute: string (nullable = true)    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a averiguar cuántas filas tienen el valor \"NA\" (como string) en la columna dep_time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "cuantos_NA = flightsDF\\\n",
    "                .where(F.col(\"dep_time\") == \"NA\")\\\n",
    "                .count()\n",
    "cuantos_NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, hay 857 filas que no tienen un dato válido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros según cierta lógica, por ejemplo la media de esa columna, etc). Lo más sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base\n",
    "a la cantidad de datos que tenemos. En nuestro caso, como tenemos un número considerable de filas, vamos a quitar todas las filas donde hay un NA en cualquiera de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n",
    "\n",
    "flightsLimpiado = flightsDF\n",
    "for nombreColumna in columnas_limpiar:  # para cada columna, nos quedamos con las filas que no tienen NA en esa columna\n",
    "    flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n",
    "flightsLimpiado.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora mostramos el número de filas que tiene el DataFrame `flightsLimpiado` tras eliminar todas esas filas, vemos que ha disminuido ligeramente\n",
    "pero sigue siendo un número considerable como para realizar analítica y sacar conclusiones sobre estos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/16 03:15:39 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "160748"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsLimpiado.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos eliminado los NA, vamos a convertir a tipo entero cada una de esas columnas que eran de tipo string. \n",
    "Ahora no debe haber problema ya que todas las cadenas de texto contienen dentro un número que puede ser convertido de texto a número. Vamos también a convertir la columna `arr_delay` de tipo entero a número real, necesario para los pasos posteriores donde ajustaremos un modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: int, dep_delay: int, arr_time: int, arr_delay: double, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: int, distance: int, hour: int, minute: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "flightsConvertido = flightsLimpiado\n",
    "\n",
    "for c in columnas_limpiar:\n",
    "    # método que crea una columna o reemplaza una existente\n",
    "    flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType())) \n",
    "\n",
    "flightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType()))\n",
    "flightsConvertido.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: double (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsConvertido.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a volver a mostrar las 5 primeras filas del DataFrame limpio. Aparentemente son iguales a las que ya teníamos, pero ahora\n",
    "Spark sí está tratando como enteros las columnas que deberían serlo, y si queremos podemos hacer operaciones aritméticas\n",
    "con ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|     70.0|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|    -23.0|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|     -4.0|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|    -23.0|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|     43.0|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "flightsConvertido.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5810b869bd7baccabe0a2952dd0baae1",
     "grade": false,
     "grade_id": "cell-c0cfdd1db1edaa7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Partiendo del DataFrame `flightsConvertido` que ya tiene los tipos correctos en las columnas, se pide: \n",
    "\n",
    "* Crear un nuevo DataFrame llamado `aeropuertosOrigenDF` que tenga una columna `origin` y que tenga tantas filas como aeropuertos distintos de *origen* existan. ¿Cuántas filas tiene? Almacenar dicho recuento en la variable entera `n_origen`.\n",
    "* Crear un nuevo DataFrame llamado `rutasDistintasDF` que tenga dos columnas `origin`, `dest` y que tenga tantas filas como rutas diferentes existan (es decir, como combinaciones distintas haya entre un origen y un destino). Una vez creado, contar cuántas combinaciones hay, almacenando dicho recuento en la variable entera `n_rutas`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|origin|\n",
      "+------+\n",
      "|   SEA|\n",
      "|   PDX|\n",
      "+------+\n",
      "\n",
      "Numero de filas:  2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aeropuertosOrigenDF = flightsConvertido.select(\"origin\").distinct()\n",
    "\n",
    "# Mostrar los resultados del DataFrame (opcional, para verificar)\n",
    "aeropuertosOrigenDF.show()\n",
    "\n",
    "# Contar el número de filas en el DataFrame\n",
    "n_origen = aeropuertosOrigenDF.count()\n",
    "print('Numero de filas: ',n_origen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cuántas filas tiene?**  \n",
    "\\( R: El numero de filas es 2 \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fb2ec5d49ff84edae4833eca797068b",
     "grade": false,
     "grade_id": "ejercicio-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|origin|dest|\n",
      "+------+----+\n",
      "|   SEA| RNO|\n",
      "|   SEA| DTW|\n",
      "|   SEA| CLE|\n",
      "|   SEA| LAX|\n",
      "|   PDX| SEA|\n",
      "+------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Numero de rutas:  115\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rutasDistintasDF = flightsConvertido.select(\"origin\",'dest').distinct()\n",
    "\n",
    "# Mostrar los resultados del DataFrame (opcional, para verificar)\n",
    "rutasDistintasDF.show(5)\n",
    "\n",
    "# Contar el número de filas en el DataFrame\n",
    "n_rutas = rutasDistintasDF.count()\n",
    "print('Numero de rutas: ',n_rutas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f02d6e47bb3cc84e97f31cce091f80b3",
     "grade": true,
     "grade_id": "ejercicio-1-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(n_origen == 2)\n",
    "assert(n_rutas == 115)\n",
    "assert(aeropuertosOrigenDF.count() == n_origen)\n",
    "assert(rutasDistintasDF.count() == n_rutas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9229f0b48ed644e35a731b404738edf2",
     "grade": false,
     "grade_id": "cell-2b5f0dea18728fcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "* Partiendo de nuevo de `flightsConvertido`, se pide calcular, *sólo para los vuelos que llegan con* ***retraso positivo***, el retraso medio a la llegada de dichos vuelos, para cada aeropuerto de destino. La nueva columna con el retraso medio a la llegada debe llamarse `retraso_medio`. El DF resultante debe estar **ordenado de mayor a menor retraso medio**. El código que calcule esto debería ir encapsulado en una función de Python llamada `retrasoMedio` que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el cálculo descrito anteriormente.\n",
    "\n",
    "* Una vez hecha la función, invocarla pasándole como argumento `flightsConvertido` y almacenar el resultado devuelto en la variable `retrasoMedioDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "097436083d0007a7d99d5108e1a504c9",
     "grade": false,
     "grade_id": "ejercicio-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dest='BOI', retraso_medio=64.75),\n",
       " Row(dest='HDN', retraso_medio=46.8),\n",
       " Row(dest='SFO', retraso_medio=41.193768844221104)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrasoMedio(df):  # El argumento que recibe la función es un DataFrame de Spark\n",
    "    # Escribe el código de tu función\n",
    "   # Filtrar vuelos con retraso positivo a la llegada\n",
    "    retrasadosDF = df.filter(F.col(\"arr_delay\") > 0)\n",
    "    \n",
    "    # Calcular retraso medio a la llegada por destino\n",
    "    retrasoMedioDF = (\n",
    "        retrasadosDF.groupBy(\"dest\")\n",
    "        .agg(F.avg(\"arr_delay\").alias(\"retraso_medio\"))\n",
    "        .orderBy(F.col(\"retraso_medio\").desc())\n",
    "    )\n",
    "    \n",
    "    return retrasoMedioDF\n",
    "lista = retrasoMedio(flightsConvertido).take(3)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12aa19d467a0c50adface20495b6cf35",
     "grade": true,
     "grade_id": "ejercicio-2-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lista = retrasoMedio(flightsConvertido).take(3)\n",
    "assert((lista[0].retraso_medio == 64.75) & (lista[0].dest == \"BOI\"))\n",
    "assert((lista[1].retraso_medio == 46.8) & (lista[1].dest == \"HDN\"))\n",
    "assert((round(lista[2].retraso_medio, 2) == 41.19) & (lista[2].dest == \"SFO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora invocamos a nuestra función `retrasoMedio` pasándole como argumento `flightsConvertido`. ¿Cuáles son los tres aeropuertos con mayor retraso medio? ¿Cuáles son sus retrasos medios en minutos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|dest|     retraso_medio|\n",
      "+----+------------------+\n",
      "| BOI|             64.75|\n",
      "| HDN|              46.8|\n",
      "| SFO|41.193768844221104|\n",
      "+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ESCRIBE AQUÍ TU CÓDIGO PARA MOSTRAR EL CONTENIDO DE retrasoMedioDF\n",
    "retrasoMedio(flightsConvertido).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cuáles son los tres aeropuertos con mayor retraso medio?**\n",
    "1. BOI \n",
    "2. HDN \n",
    "3. SFO \n",
    "\n",
    "**¿Cuáles son sus retrasos medios en minutos?**\n",
    "\n",
    "\\\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "\\textbf{dest} & \\textbf{retraso\\_medio} \\\\\n",
    "\\hline\n",
    "\\text{BOI} & 64.75 \\\\\n",
    "\\hline\n",
    "\\text{HDN} & 46.8 \\\\\n",
    "\\hline\n",
    "\\text{SFO} & 41.193768844221104 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa9ed55cd86eb0958e150d6a918db1af",
     "grade": false,
     "grade_id": "cell-e577747d4427e32b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 3\n",
    "\n",
    "Ajustar un modelo de DecisionTree de Spark para predecir si un vuelo vendrá o no con retraso (problema de clasificación binaria), utilizando como variables predictoras el mes, el día del mes, la hora de partida `dep_time`, la hora de llegada `arr_time`, el tipo de avión (`carrier`), la distancia y el tiempo que permanece en el aire. Para ello, sigue los siguientes pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4405b7db7180445df5cacc66d82db53",
     "grade": false,
     "grade_id": "cell-e577747d4427e32a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notemos que en estos datos hay variables numéricas y variables categóricas que ahora mismo están tipadas como numéricas, como por ejemplo el mes del año (`month`), que es en realidad categórica. Debemos indicar a Spark cuáles son categóricas e indexarlas. Para ello se pide: \n",
    "\n",
    "* Crear un `StringIndexer` llamado `indexerMonth` y otro llamado `indexerCarrier` sobre las variables categóricas `month` y `carrier` (tipo de avión). El nombre de las columnas indexadas que se crearán debe ser, respectivamente, `monthIndexed` y `carrierIndexed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##librerias \n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------+--------------+\n",
      "|month|monthIndexed|carrier|carrierIndexed|\n",
      "+-----+------------+-------+--------------+\n",
      "|    1|        10.0|     AS|           0.0|\n",
      "|    1|        10.0|     US|           6.0|\n",
      "|    1|        10.0|     UA|           4.0|\n",
      "|    1|        10.0|     US|           6.0|\n",
      "|    1|        10.0|     AS|           0.0|\n",
      "|    1|        10.0|     DL|           3.0|\n",
      "|    1|        10.0|     UA|           4.0|\n",
      "|    1|        10.0|     UA|           4.0|\n",
      "|    1|        10.0|     UA|           4.0|\n",
      "|    1|        10.0|     UA|           4.0|\n",
      "|    1|        10.0|     UA|           4.0|\n",
      "|    1|        10.0|     US|           6.0|\n",
      "|    1|        10.0|     DL|           3.0|\n",
      "|    1|        10.0|     AA|           5.0|\n",
      "|    1|        10.0|     AS|           0.0|\n",
      "|    1|        10.0|     AS|           0.0|\n",
      "|    1|        10.0|     F9|           9.0|\n",
      "|    1|        10.0|     AA|           5.0|\n",
      "|    1|        10.0|     AS|           0.0|\n",
      "|    1|        10.0|     F9|           9.0|\n",
      "+-----+------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un StringIndexer para la columna 'month'\n",
    "indexerMonth = StringIndexer(inputCol=\"month\", outputCol=\"monthIndexed\")\n",
    "\n",
    "# Crear un StringIndexer para la columna 'carrier'\n",
    "indexerCarrier = StringIndexer(inputCol=\"carrier\", outputCol=\"carrierIndexed\")\n",
    "\n",
    "# Aplicar el indexer de 'month'\n",
    "flightsIndexado = indexerMonth.fit(flightsConvertido).transform(flightsConvertido)\n",
    "\n",
    "\n",
    "# Aplicar el indexer de 'carrier'\n",
    "flightsIndexado = indexerCarrier.fit(flightsIndexado).transform(flightsIndexado)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar el resultado\n",
    "flightsIndexado.select(\"month\", \"monthIndexed\", \"carrier\", \"carrierIndexed\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb039584cd14e6bc3434e5be930341e6",
     "grade": false,
     "grade_id": "string-indexer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c96c7c7d5836922dd549b358b897b781",
     "grade": true,
     "grade_id": "string-indexer-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(indexerMonth, StringIndexer))\n",
    "assert(isinstance(indexerCarrier, StringIndexer))\n",
    "assert(indexerMonth.getInputCol() == \"month\")\n",
    "assert(indexerMonth.getOutputCol() == \"monthIndexed\")\n",
    "assert(indexerCarrier.getInputCol() == \"carrier\")\n",
    "assert(indexerCarrier.getOutputCol() == \"carrierIndexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69e024b0baeeb36bb74d136c7e113372",
     "grade": false,
     "grade_id": "cell-e577747d4427e323",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recordemos también que Spark requiere que todas las variables estén en una única columna de tipo vector, por lo que después de indexar estas dos variables, tendremos que fusionar en una columna de tipo vector todas ellas, utilizando un `VectorAssembler`. Se pide:\n",
    "\n",
    "* Crear en una variable llamada `vectorAssembler` un `VectorAssembler` que reciba como entrada una lista de todas las variables de entrada (y que no debe incluir `arr_delay`) que serán las que formarán parte del modelo. Crear primero esta lista de variables (lista de strings) en la variable `columnas_ensamblar` y pasar dicha variable como argumento al crear el `VectorAssembler`. Como es lógico, en el caso de las columnas `month` y `carrier`, no usaremos las variables originales sino las indexadas en el apartado anterior. La columna de tipo vector creada con las características ensambladas debe llamarse `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44623471d64d0605fcbcd4bbcc3c2a0d",
     "grade": false,
     "grade_id": "vector-assembler",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "+----------------------------------------+---------+\n",
      "|features                                |arr_delay|\n",
      "+----------------------------------------+---------+\n",
      "|[1.0,235.0,1542.0,194.0,0.0,10.0,0.0]   |70.0     |\n",
      "|[4.0,738.0,2279.0,252.0,0.0,10.0,6.0]   |-23.0    |\n",
      "|[8.0,548.0,1825.0,201.0,0.0,10.0,4.0]   |-4.0     |\n",
      "|[28.0,800.0,2282.0,251.0,0.0,10.0,6.0]  |-23.0    |\n",
      "|[34.0,325.0,1448.0,201.0,0.0,10.0,0.0]  |43.0     |\n",
      "|[37.0,747.0,1927.0,224.0,0.0,10.0,3.0]  |88.0     |\n",
      "|[346.0,936.0,1721.0,202.0,3.0,10.0,4.0] |219.0    |\n",
      "|[526.0,1148.0,1825.0,217.0,5.0,10.0,4.0]|15.0     |\n",
      "|[527.0,917.0,1024.0,136.0,5.0,10.0,4.0] |24.0     |\n",
      "|[536.0,1334.0,2402.0,268.0,5.0,10.0,4.0]|-6.0     |\n",
      "|[541.0,911.0,991.0,130.0,5.0,10.0,4.0]  |4.0      |\n",
      "|[549.0,907.0,1009.0,122.0,5.0,10.0,6.0] |12.0     |\n",
      "|[550.0,837.0,689.0,82.0,5.0,10.0,3.0]   |-12.0    |\n",
      "|[557.0,1134.0,1660.0,184.0,5.0,10.0,5.0]|-16.0    |\n",
      "|[557.0,825.0,1448.0,188.0,5.0,10.0,0.0] |-25.0    |\n",
      "|[558.0,801.0,697.0,100.0,5.0,10.0,0.0]  |-2.0     |\n",
      "|[559.0,916.0,991.0,125.0,5.0,10.0,9.0]  |-9.0     |\n",
      "|[600.0,1151.0,1721.0,206.0,6.0,10.0,5.0]|-19.0    |\n",
      "|[600.0,842.0,954.0,125.0,6.0,10.0,0.0]  |-8.0     |\n",
      "|[602.0,943.0,1024.0,129.0,6.0,10.0,9.0] |5.0      |\n",
      "+----------------------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Crear la lista de columnas a ensamblar, excluyendo 'arr_delay' y usando las columnas indexadas\n",
    "input_cols = [\n",
    "    \"dep_time\", \"arr_time\", \"distance\",\"air_time\", \"hour\",\"monthIndexed\", \"carrierIndexed\"\n",
    "]\n",
    "\n",
    "print(len(input_cols))\n",
    "# Crear el VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "\n",
    "# Transformar el DataFrame para crear la columna 'features'\n",
    "flightsFinal = vectorAssembler.transform(flightsIndexado)\n",
    "\n",
    "# Mostrar el DataFrame resultante con la columna 'features'\n",
    "flightsFinal.select(\"features\", \"arr_delay\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d17b2fa1d8a4bd02b89952429ba1552",
     "grade": true,
     "grade_id": "vector-assembler-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(vectorAssembler, VectorAssembler))\n",
    "assert(vectorAssembler.getOutputCol() == \"features\")\n",
    "input_cols = vectorAssembler.getInputCols()\n",
    "assert(len(input_cols) == 7)\n",
    "assert(\"arr_delay\" not in input_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8eeb3a3f0b15e8b374789706cd9bce49",
     "grade": false,
     "grade_id": "cell-e577747d4427e32dsdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finalmente, vemos que la columna `arr_delay` es continua, y no binaria como requiere un problema de clasificación con dos clases. Vamos a convertirla en binaria. Para ello se pide:\n",
    "\n",
    "* Utilizar un binarizador de Spark, fijando a 15 el umbral, y guardarlo en la variable `delayBinarizer`. Consideramos retrasado un vuelo que ha llegado con más de 15 minutos de retraso, y no retrasado en caso contrario. La nueva columna creada con la variable binaria debe llamarse `arr_delay_binary` y debe ser interpretada como la columna target para nuestro algoritmo. Por ese motivo, esta columna **no** se incluyó en el apartado anterior entre las columnas que se ensamblan para formar las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f33c493a3c3dd83fb6a39a7acefca5c",
     "grade": false,
     "grade_id": "binarizer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|arr_delay|arr_delay_binary|\n",
      "+---------+----------------+\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "|     16.0|             1.0|\n",
      "+---------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "# Crear el Binarizer\n",
    "delayBinarizer = Binarizer(threshold=15, inputCol=\"arr_delay\", outputCol=\"arr_delay_binary\")\n",
    "\n",
    "# Transformar el DataFrame para agregar la columna binaria\n",
    "flightsBinarizado = delayBinarizer.transform(flightsFinal)\n",
    "\n",
    "# Mostrar el DataFrame resultante con la nueva columna binaria\n",
    "flightsBinarizado.select(\"arr_delay\", \"arr_delay_binary\").where(F.col('arr_delay')==16).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ec408ab263c0378526f96bb5d374704",
     "grade": true,
     "grade_id": "binarizer-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(delayBinarizer, Binarizer))\n",
    "assert(delayBinarizer.getThreshold() == 15)\n",
    "assert(delayBinarizer.getInputCol() == \"arr_delay\")\n",
    "assert(delayBinarizer.getOutputCol() == \"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5aed16d7ed0219fe1d8741848f594319",
     "grade": false,
     "grade_id": "cell-25a7793978ee7d05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Por último, crearemos el modelo de clasificación.\n",
    "\n",
    "* Crear en una variable `decisionTree` un árbol de clasificación de Spark (`DecisionTreeClassifier` del paquete `pyspark.ml.classification`)\n",
    "* Indicar como columna de entrada la nueva columna creada por el `VectorAssembler` creado en un apartado anterior.\n",
    "* Indicar como columna objetivo (target) la nueva columna creada por el `Binarizer` del apartado anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e785136d6d06691c003ff9542027e03d",
     "grade": false,
     "grade_id": "decision-tree",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Crear el árbol de clasificación\n",
    "decisionTree = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",        # Columna de entrada generada por el VectorAssembler\n",
    "    labelCol=\"arr_delay_binary\"    # Columna objetivo creada por el Binarizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b03b48f304b5b34cd06eeec49e001fd",
     "grade": true,
     "grade_id": "decision-tree-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(decisionTree, DecisionTreeClassifier))\n",
    "assert(decisionTree.getFeaturesCol() == \"features\")\n",
    "assert(decisionTree.getLabelCol() == \"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "728da91edabbac5b62baf31bdd0a707e",
     "grade": false,
     "grade_id": "cell-e577747d4427e32d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora vamos a encapsular todas las fases en un sólo pipeline y procederemos a entrenarlo. Se pide:\n",
    "\n",
    "* Crear en una variable llamada `pipeline` un objeto `Pipeline` de Spark con las etapas anteriores en el orden adecuado para poder entrenar un modelo. \n",
    "\n",
    "* Entrenarlo invocando sobre ella al método `fit` y guardar el pipeline entrenado devuelto por dicho método en una variable llamada `pipelineModel`. \n",
    "\n",
    "* Aplicar el pipeline entrenado para transformar (predecir) el DataFrame `flightsConvertido`, guardando las predicciones devueltas en la variable `flightsPredictions` que será un DataFrame. Nótese que estamos prediciendo los propios datos de entrenamiento y que, por simplicidad, no habíamos hecho (aunque habría sido lo correcto) ninguna división de nuestros datos originales en subconjuntos distintos de entrenamiento y test antes de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edbdb627305d03efa41a88426330e160",
     "grade": false,
     "grade_id": "pipeline",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------+----------+----------------------------------------+\n",
      "|features                                |arr_delay_binary|prediction|probability                             |\n",
      "+----------------------------------------+----------------+----------+----------------------------------------+\n",
      "|[1.0,235.0,1542.0,194.0,0.0,10.0,0.0]   |1.0             |0.0       |[0.6401468788249693,0.3598531211750306] |\n",
      "|[4.0,738.0,2279.0,252.0,0.0,10.0,6.0]   |0.0             |0.0       |[0.6401468788249693,0.3598531211750306] |\n",
      "|[8.0,548.0,1825.0,201.0,0.0,10.0,4.0]   |0.0             |0.0       |[0.6401468788249693,0.3598531211750306] |\n",
      "|[28.0,800.0,2282.0,251.0,0.0,10.0,6.0]  |0.0             |0.0       |[0.6401468788249693,0.3598531211750306] |\n",
      "|[34.0,325.0,1448.0,201.0,0.0,10.0,0.0]  |1.0             |0.0       |[0.6401468788249693,0.3598531211750306] |\n",
      "|[37.0,747.0,1927.0,224.0,0.0,10.0,3.0]  |1.0             |0.0       |[0.6401468788249693,0.3598531211750306] |\n",
      "|[346.0,936.0,1721.0,202.0,3.0,10.0,4.0] |1.0             |1.0       |[0.02,0.98]                             |\n",
      "|[526.0,1148.0,1825.0,217.0,5.0,10.0,4.0]|0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[527.0,917.0,1024.0,136.0,5.0,10.0,4.0] |1.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[536.0,1334.0,2402.0,268.0,5.0,10.0,4.0]|0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[541.0,911.0,991.0,130.0,5.0,10.0,4.0]  |0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[549.0,907.0,1009.0,122.0,5.0,10.0,6.0] |0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[550.0,837.0,689.0,82.0,5.0,10.0,3.0]   |0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[557.0,1134.0,1660.0,184.0,5.0,10.0,5.0]|0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[557.0,825.0,1448.0,188.0,5.0,10.0,0.0] |0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[558.0,801.0,697.0,100.0,5.0,10.0,0.0]  |0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[559.0,916.0,991.0,125.0,5.0,10.0,9.0]  |0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[600.0,1151.0,1721.0,206.0,6.0,10.0,5.0]|0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[600.0,842.0,954.0,125.0,6.0,10.0,0.0]  |0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "|[602.0,943.0,1024.0,129.0,6.0,10.0,9.0] |0.0             |0.0       |[0.9180164722031572,0.08198352779684283]|\n",
      "+----------------------------------------+----------------+----------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Crear el Pipeline con las etapas en el orden adecuado\n",
    "pipeline = Pipeline(stages=[indexerMonth, indexerCarrier, vectorAssembler, delayBinarizer, decisionTree])\n",
    "\n",
    "# Entrenar el modelo (fit) con el DataFrame original\n",
    "pipelineModel = pipeline.fit(flightsConvertido)\n",
    "\n",
    "# Aplicar el pipeline entrenado para hacer predicciones\n",
    "flightsPredictions = pipelineModel.transform(flightsConvertido)\n",
    "\n",
    "# Mostrar las predicciones\n",
    "flightsPredictions.select(\"features\", \"arr_delay_binary\", \"prediction\", \"probability\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "107b5ec260550eb4235ffecff655289a",
     "grade": true,
     "grade_id": "pipeline-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "assert(isinstance(pipeline, Pipeline))\n",
    "assert(len(pipeline.getStages()) == 5)\n",
    "assert(isinstance(pipelineModel, PipelineModel))\n",
    "assert(\"probability\" in flightsPredictions.columns)\n",
    "assert(\"prediction\" in flightsPredictions.columns)\n",
    "assert(\"rawPrediction\" in flightsPredictions.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c531953abf18cfb3b67571ddde7a57d",
     "grade": false,
     "grade_id": "cell-61156fe5938763f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Vamos a mostrar la matriz de confusión (este apartado no es evaluable). Agrupamos por la variable que tiene la clase verdadera y la que tiene la clase predicha, para ver en cuántos casos coinciden y en cuántos difieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75f98ae39b827e75c3f0b4b2aaa6b0db",
     "grade": false,
     "grade_id": "cell-896752beb71cb455",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------+\n",
      "|arr_delay_binary|prediction| count|\n",
      "+----------------+----------+------+\n",
      "|             1.0|       1.0|   655|\n",
      "|             0.0|       1.0|   129|\n",
      "|             1.0|       0.0| 23594|\n",
      "|             0.0|       0.0|136370|\n",
      "+----------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 69:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8524211809789235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Convertir las predicciones y etiquetas en un RDD\n",
    "predictionAndLabels = flightsPredictions.select(\"prediction\", \"arr_delay_binary\").rdd\n",
    "\n",
    "# Crear el objeto de métricas multicategoría\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Obtener el accuracy\n",
    "accuracy = metrics.accuracy\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis del Modelo\n",
    "\n",
    "\n",
    "El modelo tiene un **accuracy aceptable de 85%**, lo cual puede resultar un poco engañoso. Esto se debe a que el modelo es bueno para predecir los vuelos **sin retraso (0.0)**, como lo demuestra el alto número de verdaderos negativos (136370).\n",
    "\n",
    "### Debilidades\n",
    "El modelo tiene dificultades para identificar vuelos **con retraso (1.0)**, lo que se refleja en:\n",
    "- Un bajo **Recall**: 2.7%.\n",
    "- Un bajo **F1-Score**: 5.3%.\n",
    "\n",
    "### Recomendaciones\n",
    "1. Investigar posibles **desbalances en los datos** (la mayoría de los vuelos no tienen retraso).  \n",
    "2. Aplicar técnicas de balanceo como:  \n",
    "   - **Sobremuestreo (oversampling)** de la clase minoritaria (retrasos).  \n",
    "   - **Submuestreo (undersampling)** de la clase mayoritaria.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
